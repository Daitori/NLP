{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9e_Rmpt0B_Oa"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models \n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel,LsiModel\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def force_format(texts):\n",
        "    return [str(t) for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_json(\"News_Category_Dataset_v2.json\", lines=True, dtype={\"headline\": str})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = force_format(dataset[\"headline\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tFCI60p8GyQ6"
      },
      "outputs": [],
      "source": [
        "def compute_word_occurences(texts):\n",
        "    words = itertools.chain.from_iterable(texts)\n",
        "    word_count = pd.Series(words).value_counts()\n",
        "    word_count = pd.DataFrame({\"Word\": word_count.index, \"Count\": word_count.values})\n",
        "    return word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_l_texts(text_file):\n",
        "    l_texts=[]\n",
        "    with open(text_file, \"r\") as f:\n",
        "        line = f.readlines()\n",
        "        list_line = [l.strip() for l in line]\n",
        "        for l in list_line:\n",
        "            l_texts.append(ast.literal_eval(l))\n",
        "    return l_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-12-07T10:50:05.882854Z",
          "start_time": "2020-12-07T10:49:42.724726Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vov7lwqCG4O",
        "outputId": "9fcf3553-3f37-4be6-f4eb-09c5bc6e6c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['mass_shooting', 'texas', 'week', 'tv'], ['smith', 'join', 'diplo', 'nicky', 'jam', 'world_cup', 'official', 'song'], ['hugh', 'grant', 'marries', 'time', 'age'], ['jim_carrey', 'blasts', 'castrato', 'adam', 'schiff', 'democrats', 'artwork'], ['julianna', 'margulie', 'donald', 'poop', 'bag', 'pick', 'dog'], ['morgan_freeman', 'devastate', 'sexual_harassment', 'claim', 'undermine', 'legacy'], ['donald', 'lovin', 'mcdonald', 'jingle', 'tonight', 'bit'], ['watch', 'amazon', 'prime', 'week'], ['mike', 'myers', 'reveal', 'fourth', 'austin', 'power', 'film'], ['watch', 'hulu', 'week']]\n"
          ]
        }
      ],
      "source": [
        "l_texts = get_l_texts(\"l_texts.txt\")\n",
        "print(l_texts[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eMxYmxOktWw"
      },
      "source": [
        "# LSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NvDsjgk3P3EE"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary\n",
        "dictionary = corpora.Dictionary(l_texts)\n",
        "\n",
        "# Create a corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in l_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of topics:  55  Coherence Score:  0.23581617191033896\n"
          ]
        }
      ],
      "source": [
        "result={\"num_topics\":[], \"coherence_score\":[]}\n",
        "lsa_num_topics= 55\n",
        "lsi_model = LsiModel(corpus, num_topics=lsa_num_topics, id2word=dictionary)\n",
        "coherence_model= CoherenceModel(model=lsi_model, texts=l_texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(\"Number of topics: \", lsa_num_topics, \" Coherence Score: \", coherence_score)\n",
        "result[\"num_topics\"].append(lsa_num_topics)\n",
        "result[\"coherence_score\"].append(coherence_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the coherence scores for elbow method\n",
        "\n",
        "plt.plot(result[\"num_topics\"], result[\"coherence_score\"])\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence Score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LsiModel' object has no attribute 'inference'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgensim_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlsi_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\David-Desktop\\miniconda3\\envs\\NLP\\lib\\site-packages\\pyLDAvis\\gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(\u001b[43m_extract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_dist\u001b[49m\u001b[43m)\u001b[49m, kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyLDAvis\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
            "File \u001b[1;32mc:\\Users\\David-Desktop\\miniconda3\\envs\\NLP\\lib\\site-packages\\pyLDAvis\\gensim_models.py:49\u001b[0m, in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     47\u001b[0m         gamma \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39minference(corpus)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m         gamma, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m(corpus)\n\u001b[0;32m     50\u001b[0m     doc_topic_dists \u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m/\u001b[39m gamma\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'LsiModel' object has no attribute 'inference'"
          ]
        }
      ],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim_models.prepare(lsi_model, corpus, dictionary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
